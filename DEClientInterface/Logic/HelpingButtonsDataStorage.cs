// DataExcavator, Version=2.2.0.0, Culture=neutral, PublicKeyToken=null
// DEClientInterface.Logic.HelpingButtonsDataStorage
using System.Collections.Generic;
using System.Windows;
using DEClientInterface.UIWindows;

namespace DEClientInterface.Logic
{
	public class HelpingButtonsDataStorage
	{
		private static readonly Dictionary<string, string> HelpingInformation = new Dictionary<string, string>
		{
			{ "ProjectTaskDirectory", "Please specify a working directory of the project. It will store all information about the project. This information includes the content of scanned pages, scraper results, page parsing templates, project settings, and so on. Please note that the application will actively access the contents of this folder - the folder size may be quite large." },
			{ "PrimaryCrawlingWay", "There are two principial ways to crawl data - .NET-scanning and CEF-scanning. .NET-scanning is based on .NET Framework classes and uses sockets to access web resources. CEF scanning uses the Chromium Embedded Framework (Chromium web browser) to access web resources. .NET scanning does not handle JS scripts or other ActiveX components. .NET scanning is slightly faster, spends considerably less resources and is suitable for simple sites. CEF scanning uses Chromium browser. It allows to load pages in the state of full readiness, including processing of all JS scripts and other ActiveX components. CEF scanning also allows you to run your own JS scripts and work with complex sites. CEF scanning is slower and spends considerably more resources. In addition, CEF scanning means using cookies, while .NET scanning does not allow them to be used. We recommend that you use the default CEF scan as it is universally suited to any task. .NET scans should only be used for very simple sites if you are convinced that this will work." },
			{ "RobotsTxtUserAgentRespectationChain", "Any robots.txt file contains a set of directives for different search engines. This parameter offers you to specify the list of search engines, according to which the search for the appropriate block of instructions in the file robots.txt will be carried out. For example, if you specify \"yandex,google\", the indexer will first search for a set of directives for yandex, and then a set of directives for google. In the sequential search, the set of directives for the first search engine found in the file specified in this parameter will be applied." },
			{ "CrawlingThreadsCount", "The number of parallel threads for the pages crawler. Each thread will scan pages from a common list of links. Increasing the number of threads leads to faster loading of pages from the site. Do not forget about possible blocking from the site side. If you will download pages too quickly - the site may limit access by IP or account. It is not recommended to specify extremely large values - try to start with 2-3 threads, focusing on the number of logical processors in your system." },
			{ "CrawlingThreadDelayMilliseconds", "The time of the thread's inactivity after each page loading is completed. This parameter is necessary to artificially slow down the thread. This parameter is specified in milliseconds." },
			{ "PagesToCrawlLimit", "Total limit of pages to be scanned. When this number of pages is reached, scanning will stop. -1 means no limits." },
			{ "PageDownloadTimeoutMilliseconds", "One page load timeout in milliseconds." },
			{ "PageDownloadAttempts", "Number of attempts to download the source code of the page. When this number of attempts is reached, the result will be marked as unsuccessful and the page scanning will be stopped." },
			{ "LinksBufferHDDAutoSavingMilliseconds", "The link buffer, which is used by the system to store information about indexed links and links waiting to be indexed. We will save this buffer on the hard drive every n milliseconds. This parameter defines the interval of resetting the buffer of references to the hard disk." },
			{ "ConcurrentCollectionsParallelismQuantity", "The system uses parallelism to analyze different data arrays. At the same time, the parallelism is used not only for data indexing but also for processing the results. This parameter defines the limit value of parallelism for relatively easy calculations related to the analysis of references or small sets of data received from scanned pages. Note that here we mean parallelism in auxiliary algorithms. The primary number of threads is determined by another parameter. It is not recommended to set this parameter to more than 4." },
			{ "HTTPWebRequestProxiesList", "Some sites are actively blocking scraping attempts. Proxy servers should be used to correctly interact with such sites. Our application allows you to define the list of proxy servers that will be used to scan a site." },
			{ "CrawlingServerProxiesRotation", "Proxy rotation is necessary if you use more than one proxy server. If rotation is enabled, the application will shuffle proxy servers from the list to use them all, not just one from the list. The shuffling will be performed using the selected algorithm - consecutive rotation or random rotation." },
			{ "CEFCrawlingBehaviors", "[ATTENTION!!! CEF Behaviors it's a difficult part of the system. Please, read FAQ first.] In the process of scanning pages you may encounter a situation where some site cannot be scanned by native methods. Such sites include sites with authorization, sites with dynamic content, sites with generated content, sites with endless scroll, and so on. In such situations, for the correct scanning of the site requires direct interaction between the application and the indexed page. In this case, you should use CEFCrawling with the Behaviors suite. The set of Behaviors allows you to define the rules of interaction with the page, such as the user's javascript code, which will be sent to the downloaded page and executed. More information about the use of the Behaviors suite can be found on the our website at FAQ page." },
			{ "CrawlingArguments", "During the process of scanning sites you may need a set of additional arguments that the scanner will pass to the pages of the sites. These arguments include $_GET and $_POST arguments. Using the given parameter, you can define the list of arguments which will be transferred to a site at scanning of pages. These arguments will be applied to all scanned pages." },
			{ "GrabbingThreadsCount", "The number of parallel threads for the page scraper. Each thread will scrape data from a common list of downloaded pages." },
			{ "PageToScrapeHTMLMask", "Enter a substring here from the source code of the target page. If this string is found on the page, the data will be scraped by the robot." },
			{ "CEFAuthBehavior", "Sometimes websites require authentication. This usually means that you must enter your login and password and press a «Login» button. Our application allows you to specify your own authentication script. Using this opportunity, you can scrape any data from all websites that require authentication. Note that you can only use this behavior when «Crawler engine» is setted to CEFCrawling." },
			{ "CEFAuthRespectedURLs", "The list of URLs (or URL substrings) where the application will check your authentication under your account. If the URL of some page contains a substring from this field, the application will check your authentication. This check will be done before any other action with the downloaded page. Typically, these links will match the RespectingURLs from the general settings of the crawling server." },
			{ "CEFAuthRestrictedURLs", "A list of URLs that will must be ignored to verify your authentication. Typically, these links match the RestrictedURLs from the common settings of the crawling server. Usually, these are links like images, account page, tray, checkout page etc." },
			{ "CEFAuthLoginPageAddress", "Link to the website authentication page. If the login page is presented on all pages of the site - then a link to any page." },
			{ "CEFAuthBehaviorLoggedOnSubstring", "A substring of HTML page layout that determines whether you are logged in to the system or not. If this substring is found on the page, it means you are logged in to the system with a specific login and password. If it is not - the application will try to login in the system in the way you have specified." },
			{ "CEFAuthBehaviorLoginScriptItself", "JS script for site authentication. In this script fields \"Login\" and \"Password\" should be filled in, and then the button \"Login\" should be pressed." },
			{ "CEFAuthBehaviorWaitBeforeAndAfterLogin", "The amount of time in seconds during which a thread will wait before executing the login script and after executing the login script." },
			{ "PatternPagesUrlsSubstrings", "A URL substring that defines the page to be scraped. For example, some site sells TVs. All pages with TVs have a link in the form https://cooltvshop/purchasetv/samsung-25inches-supercool. In this case you can specify here something like: purchase, purchasetv, purchasetv/. Thus, the application will only extract data from those pages that contain the lines: purchase, purchasetv, purchasetv/." },
			{ "ParsingPatternOuterSelector", "Outer data selector. If you add an external selector to the pattern, the grabber first selects all the blocks corresponding to that selector from the page, and then applies a set of internal rules to each block. The external pattern selector will usually be useful for pages that contain data lists - complex block sets, tiles, cards, etc. It is not recommended to use an external selector for standard product pages (one product - one page) or any pages described in one-dimensional or linear manner." },
			{ "ParseBinaryAttributes", "A directive that forces the server to collect some specified attributes and their values. This directive is useful for downloading content elements of the site - images, binary files, etc. The usual application of this directive is to download images." },
			{ "ElementAttributesList", "Settings of attributes to be downloaded, according to which data collection and analysis will be carried out." },
			{ "ExpandIFramesSelectBox", "Sometimes you may need to extract data from a page that contains iframe blocks. If you set this value to true, the system will download iframe content and place it on the page within the corresponding iframe blocks. Using this option, you will be able to access iframe content through .CSS selectors or XPath." },
			{ "CAPTCHABehavior", "Some sites are actively protected against scraping by using CAPTCHA. This usually looks like a window with an image that needs to be rewritten into an input box. Sometimes it looks like a set of images, some of which need to be clicked. In such cases it is impossible to extract content from the site without using automatic CAPTCHA solving. Our application allows you to configure automatic CAPTCHA solving with the appropriate functionality." },
			{ "CAPTCHASOLVER_2CaptchaAPIKey", "API key to connect to the service https://2captcha.com. The key can be found on the main page of your personal account: https://2captcha.com/enterpage" },
			{ "CAPTCHASOLVER_CaptchaType", "The type of CAPTCHA that is used on the website whose data you are retrieving. It is very important that you correctly specify the type of CAPTCHA they are using. If it will be specified uncorrectly, the recognition will NOT work." },
			{ "CAPTCHASOLVER_CaptchaDetectionSubstring", "Specify the line that will be used to identify CAPTCHA on the page. If this string is found in the source code of the pages, then we will use the CAPTCHA search and processing mechanism. If this string is NOT found on the page, we will process the page in the standard way." },
			{ "CAPTCHASOLVER_CaptchaResolverAttempts", "The number of attempts to process CAPTCHA, on reaching which the process will be completed with an error." },
			{ "CAPTCHASOLVER_CaptchaResolverTimeout", "Time in seconds for the entire CAPTCHA solving process. When this time is reached, the processing will end with an error." },
			{ "CAPTCHASOLVER_CaptchaResolverIddleTime", "Idle time after successful or unsuccessful CAPTCHA processing." }
		};

		public static Window ShowHelpWindow(string HelpingInformationCode)
		{
			string textInformation = HelpingInformation[HelpingInformationCode];
			DEHelpButtonInformation dEHelpButtonInformation = new DEHelpButtonInformation(textInformation);
			dEHelpButtonInformation.WindowStartupLocation = WindowStartupLocation.CenterScreen;
			dEHelpButtonInformation.Show();
			return dEHelpButtonInformation;
		}
	}
}